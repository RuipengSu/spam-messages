{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8 \n",
    "from numpy import *\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 加载原始数据，进行分割\n",
    "def load_message():\n",
    "    content = []\n",
    "    lines = []\n",
    "    label = []\n",
    "\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_train.txt', encoding = 'utf-8') as fr:\n",
    "        for i in range(10000):\n",
    "            line = fr.readline()\n",
    "            lines.append(line)\n",
    "        num = len(lines)\n",
    "        for i in range(num):\n",
    "            message = lines[i].split('\\t')\n",
    "            label.append(message[0])\n",
    "            content.append(message[1])\n",
    "\n",
    "    return num, content, label\n",
    "\n",
    "\n",
    "# 将分割后的原始数据存到json\n",
    "def data_storage(content, label):\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_content.json', 'w') as f:\n",
    "        json.dump(content, f)\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'w') as f:\n",
    "        json.dump(label, f)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "   num, content, label = load_message()\n",
    "   data_storage(content, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\surui\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.801 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import sklearn.feature_extraction.text\n",
    "import json\n",
    "import re\n",
    "from scipy import sparse, io\n",
    "\n",
    "\n",
    "# 将连续的数字转变为长度的维度\n",
    "def process_cont_numbers(content):\n",
    "    digits_features = np.zeros((len(content), 16))\n",
    "    for i, line in enumerate(content):\n",
    "        for digits in re.findall(r'\\d+', line):\n",
    "            length = len(digits)\n",
    "            if 0 < length <= 15:\n",
    "                digits_features[i, length-1] += 1\n",
    "            elif length > 15:\n",
    "                digits_features[i, 15] += 1\n",
    "    return process_cont_numbers\n",
    "\n",
    "\n",
    "# 正常分词，非TFID\n",
    "class MessageCountVectorizer(sklearn.feature_extraction.text.CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        def analyzer(doc):\n",
    "            words = pseg.cut(doc)\n",
    "            new_doc = ''.join(w.word for w in words if w.flag != 'x')\n",
    "            words = jieba.cut(new_doc)\n",
    "            return words\n",
    "        return analyzer\n",
    "\n",
    "\n",
    "# 用TFID生成对应词向量\n",
    "class TfidfVectorizer(sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        #analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        def analyzer(doc):\n",
    "            words = pseg.cut(doc)\n",
    "            new_doc = ''.join(w.word for w in words if w.flag != 'x')\n",
    "            words = jieba.cut(new_doc)\n",
    "            return words\n",
    "        return analyzer\n",
    "\n",
    "\n",
    "# 生成词向量并进行存储\n",
    "def vector_word():\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_content.json', 'r') as f:\n",
    "        content = json.load(f)\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    '''\n",
    "        vec_count = MessageCountVectorizer(min_df=2, max_df=0.8)\n",
    "        data_count = vec_count.fit_transform(content)\n",
    "        name_count_feature = vec_count.get_feature_names()\n",
    "    '''\n",
    "\n",
    "    vec_tfidf = TfidfVectorizer(min_df = 2, max_df = 0.8)\n",
    "    data_tfidf = vec_tfidf.fit_transform(content)\n",
    "    name_tfidf_feature = vec_tfidf.get_feature_names()\n",
    "\n",
    "    io.mmwrite('D:\\Document\\CS\\Program\\py program\\spam messages\\word_vector.mtx', data_tfidf)\n",
    "\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'w') as f:\n",
    "        json.dump(label, f)\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_vector_type.json', 'w') as f:\n",
    "        json.dump(name_tfidf_feature, f)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    vector_word()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import json\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse, io\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def dimensionality_reduction(training_data, test_data, type='pca'):\n",
    "    if type == 'pca':\n",
    "        n_components = 1000\n",
    "        t0 = time()\n",
    "        pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True)\n",
    "        pca.fit(training_data)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        t0 = time()\n",
    "        training_data_transform = sparse.csr_matrix(pca.transform(training_data))\n",
    "        test_data_transform = sparse.csr_matrix(pca.transform(test_data))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        #random_projections\n",
    "        #feature_agglomeration\n",
    "        return training_data_transform, test_data_transform\n",
    "\n",
    "\n",
    "\n",
    "def split_data(content, label):\n",
    "    training_data, test_data, training_target, test_target = train_test_split(\n",
    "        content, label, test_size=0.1, random_state=20)\n",
    "    return training_data, test_data, training_target, test_target\n",
    "\n",
    "def standardized_data(content, label):\n",
    "    training_data, test_data, training_target, test_target = split_data(content, label)\n",
    "    scalar = preprocessing.StandardScaler().fit(training_data)\n",
    "    training_data_transformed = scalar.transform(training_data)\n",
    "    test_data_transformed = scalar.transform(test_data)\n",
    "    return training_data_transformed, test_data_transformed, training_target, test_target\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "\n",
    "class TrainerLinear:\n",
    "    def __init__(self, training_data, training_target):\n",
    "        self.training_data = training_data\n",
    "        self.training_target = training_target\n",
    "        self.clf = svm.SVC(C=1, class_weight=None, coef0=0.0,\n",
    "                           decision_function_shape=None, degree=3, gamma='auto',\n",
    "                           kernel='linear', max_iter=-1, probability=False,\n",
    "                           random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    def learn_best_param(self):\n",
    "        C_range = np.logspace(-2, 10, 13)\n",
    "        param_grid = dict(C=C_range)\n",
    "        cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "        grid.fit(self.training_data, self.training_target)\n",
    "        self.clf.set_params(C=grid.best_params_['C'])\n",
    "        print(\"The best parameters are %s with a score of %0.5f\"\n",
    "              % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "    def train_classifier(self):\n",
    "        self.clf.fit(self.training_data, self.training_target)\n",
    "        joblib.dump(self.clf, 'D:\\Document\\CS\\Program\\py program\\spam messages\\SVM_linear_estimator.pkl')\n",
    "        training_result = self.clf.predict(self.training_data)\n",
    "        print (metrics.classification_report(self.training_target, training_result))\n",
    "\n",
    "    def cross_validation(self):\n",
    "        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=20)\n",
    "        scores = cross_val_score(self.clf, self.training_data, self.training_target, cv=cv, scoring='f1_macro')\n",
    "        print (scores)\n",
    "        print(\"f1 score: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "class TrainerRbf:\n",
    "    def __init__(self, training_data, training_target):\n",
    "        self.training_data = training_data\n",
    "        self.training_target = training_target\n",
    "        self.clf = svm.SVC(C=100, class_weight=None, coef0=0.0,\n",
    "                           decision_function_shape=None, degree=3, gamma=0.01,\n",
    "                           kernel='rbf', max_iter=-1, probability=False,\n",
    "                           random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    def learn_best_param(self):\n",
    "        C_range = np.logspace(-2, 10, 13)\n",
    "        gamma_range = np.logspace(-9, 3, 13)\n",
    "        param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "        cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "        grid.fit(self.training_data, self.training_target)\n",
    "        self.clf.set_params(C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])\n",
    "        print(\"The best parameters are %s with a score of %0.5f\"\n",
    "              % (grid.best_params_, grid.best_score_))\n",
    "        self.draw_visualization_param_effect(grid, C_range, gamma_range)\n",
    "\n",
    "    def draw_visualization_param_effect(self, grid, C_range, gamma_range):\n",
    "        scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                             len(gamma_range))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "        plt.imshow(scores, interpolation='nearest',\n",
    "                   norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "        plt.xlabel('gamma')\n",
    "        plt.ylabel('C')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "        plt.yticks(np.arange(len(C_range)), C_range)\n",
    "        plt.title('Validation accuracy')\n",
    "        plt.savefig('D:\\Document\\CS\\Program\\py program\\spam messages\\param_effect.png')\n",
    "        plt.show()\n",
    "\n",
    "    def train_classifier(self):\n",
    "        self.clf.fit(self.training_data, self.training_target)\n",
    "        joblib.dump(self.clf, 'D:\\Document\\CS\\Program\\py program\\spam messages\\SVM_rbf_estimator.pkl')\n",
    "        training_result = self.clf.predict(self.training_data)\n",
    "        print (metrics.classification_report(self.training_target, training_result))\n",
    "\n",
    "\n",
    "    def cross_validation(self):\n",
    "        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=20)\n",
    "        scores = cross_val_score(self.clf, self.training_data, self.training_target, cv=cv, scoring='f1_macro')\n",
    "        print (scores)\n",
    "        print(\"f1 score: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SVM_train(train_data, train_target):\n",
    "    clf = svm.SVC(kernel='linear', class_weight='balanced', C =100, gamma = 0.01)\n",
    "    clf.fit(train_data, train_target)\n",
    "    expected = train_target\n",
    "    predicted = clf.predict(train_data)\n",
    "    # summarize the fit of the model\n",
    "    print (metrics.classification_report(expected, predicted))\n",
    "    print (metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "\n",
    "def feature_selection(data, data_target, feature_names):\n",
    "    clf = svm.SVC(class_weight='balanced', C=2)\n",
    "    clf.fit(data, data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 39.550s\n",
      "done in 4.536s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8128\n",
      "          1       1.00      1.00      1.00       872\n",
      "\n",
      "avg / total       1.00      1.00      1.00      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if '__main__' == __name__:\n",
    "    content = io.mmread('D:\\Document\\CS\\Program\\py program\\spam messages\\word_vector.mtx')\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    training_data, test_data, training_target, test_target = split_data(content, label)\n",
    "    training_data, test_data = dimensionality_reduction(training_data.todense(), test_data.todense(), type='pca')\n",
    "\n",
    "    Trainer = TrainerLinear(training_data, training_target)\n",
    "    #Trainer.learn_best_param()\n",
    "    Trainer.train_classifier()\n",
    "    #Trainer.cross_validation()\n",
    "\n",
    "    #Trainer2 = TrainerRbf(training_data, training_target)\n",
    "    #Trainer2.learn_best_param()\n",
    "    #Trainer2.train_classifier()\n",
    "    #Trainer2.cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    clf = joblib.load('D:\\Document\\CS\\Program\\py program\\spam messages\\SVM_linear_estimator.pkl')\n",
    "\n",
    "    def __init__(self, training_data, training_target, test_data, test_target):\n",
    "        self.trainer = TrainerLinear(training_data, training_target)\n",
    "        self.predictor = Predictor(test_data, test_target)\n",
    "\n",
    "    def train(self):\n",
    "        #self.trainer.learn_best_param()\n",
    "        self.trainer.train_classifier()\n",
    "        joblib.dump(self.clf, 'D:\\Document\\CS\\Program\\py program\\spam messages\\Terminal_estimator.pkl')\n",
    "        Evaluator.clf = joblib.load('D:\\Document\\CS\\Program\\py program\\spam messages\\Terminal_estimator.pkl')\n",
    "\n",
    "    def cross_validation(self):\n",
    "        self.trainer.cross_validation()\n",
    "\n",
    "    def predict(self, type):\n",
    "        if (type == 'sample_data'):\n",
    "            self.predictor.sample_predict(Evaluator.clf)\n",
    "        elif (type == 'new_data'):\n",
    "            self.predictor.new_predict(Evaluator.clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, test_data, test_target):\n",
    "        self.test_data = test_data\n",
    "        self.test_target = test_target\n",
    "\n",
    "    def sample_predict(self, clf):\n",
    "        test_result = clf.predict(self.test_data)\n",
    "        print (metrics.classification_report(self.test_target, test_result))\n",
    "        print (metrics.confusion_matrix(self.test_target, test_result))\n",
    "\n",
    "    def new_predict(self, clf):\n",
    "        test_result = clf.predict(self.test_data)\n",
    "        with open('D:\\Document\\CS\\Program\\py program\\spam messages\\predict_label.txt', 'wt') as f:\n",
    "            for i in range(len(test_result)):\n",
    "                f.writelines(test_result[i])\n",
    "        self.test_target = test_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 40.169s\n",
      "done in 5.468s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      8128\n",
      "          1       1.00      1.00      1.00       872\n",
      "\n",
      "avg / total       1.00      1.00      1.00      9000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       906\n",
      "          1       0.87      0.83      0.85        94\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1000\n",
      "\n",
      "[[894  12]\n",
      " [ 16  78]]\n"
     ]
    }
   ],
   "source": [
    "if '__main__' == __name__:\n",
    "    content = io.mmread('D:\\Document\\CS\\Program\\py program\\spam messages\\word_vector.mtx')\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    training_data, test_data, training_target, test_target = split_data(content, label)\n",
    "    training_data, test_data = dimensionality_reduction(training_data.todense(), test_data.todense(), type='pca')\n",
    "    evaluator = Evaluator(training_data, training_target, test_data, test_target)\n",
    "    evaluator.train()\n",
    "    #evaluator.cross_validation()\n",
    "    evaluator.predict(type='sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class Trainer_bayes:\n",
    "    def __init__(self, training_data, training_target):\n",
    "        self.training_data = training_data\n",
    "        self.training_target = training_target\n",
    "        self.clf = GaussianNB()\n",
    "\n",
    "\n",
    "    def train_classifier(self):\n",
    "        self.clf.fit(self.training_data, self.training_target)\n",
    "        joblib.dump(self.clf, 'D:\\Document\\CS\\Program\\py program\\spam messages\\spam_bayes_estimator.pkl')\n",
    "        training_result = self.clf.predict(self.training_data)\n",
    "        print (metrics.classification_report(self.training_target, training_result))\n",
    "\n",
    "    def cross_validation(self):\n",
    "        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=20)\n",
    "        scores = cross_val_score(self.clf, self.training_data, self.training_target, cv=cv, scoring='f1_macro')\n",
    "        print (scores)\n",
    "        print(\"Accuracy: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bayes_train(train_data, train_target):\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(train_data, train_target)\n",
    "    expected = train_target\n",
    "    predicted = model.predict(train_data)\n",
    "    # summarize the fit of the model\n",
    "    print (metrics.classification_report(expected, predicted))\n",
    "    print (metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 9957)\n",
      "done in 38.522s\n",
      "done in 4.727s\n",
      "(9000, 1000)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.77      0.86      8128\n",
      "          1       0.28      0.83      0.42       872\n",
      "\n",
      "avg / total       0.91      0.78      0.82      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if '__main__' == __name__:\n",
    "    content = io.mmread('D:\\Document\\CS\\Program\\py program\\spam messages\\word_vector.mtx')\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    content = content\n",
    "    training_data, test_data, training_target, test_target = split_data(content, label)\n",
    "    print (np.shape(training_data))\n",
    "    training_data, test_data = dimensionality_reduction(training_data.todense(), test_data.todense(), type='pca')\n",
    "    print (np.shape(training_data))\n",
    "\n",
    "    Trainer = Trainer_bayes(training_data.todense(), training_target)\n",
    "    Trainer.train_classifier()\n",
    "    #Trainer.cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, test_data, test_target):\n",
    "        self.test_data = test_data\n",
    "        self.test_target = test_target\n",
    "\n",
    "    def sample_predict(self, clf):\n",
    "        test_result = clf.predict(self.test_data)\n",
    "        print (metrics.classification_report(self.test_target, test_result))\n",
    "        print (metrics.confusion_matrix(self.test_target, test_result))\n",
    "\n",
    "    def new_predict(self, clf):\n",
    "        test_result = clf.predict(self.test_data)\n",
    "        with open('D:\\Document\\CS\\Program\\py program\\spam messages\\predict_label.txt', 'wt') as f:\n",
    "            for i in range(len(test_result)):\n",
    "                f.writelines(test_result[i])\n",
    "        self.test_target = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    clf = joblib.load('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_bayes_estimator.pkl')\n",
    "\n",
    "    def __init__(self, training_data, training_target, test_data, test_target):\n",
    "        self.trainer = Trainer_bayes(training_data, training_target)\n",
    "        self.predictor = Predictor(test_data, test_target)\n",
    "\n",
    "    def train(self):\n",
    "        #self.trainer.learn_best_param()\n",
    "        self.trainer.train_classifier()\n",
    "        joblib.dump(self.clf, 'D:\\Document\\CS\\Program\\py program\\spam messages\\Terminal_estimator.pkl')\n",
    "        Evaluator.clf = joblib.load('D:\\Document\\CS\\Program\\py program\\spam messages\\Terminal_estimator.pkl')\n",
    "\n",
    "    def cross_validation(self):\n",
    "        self.trainer.cross_validation()\n",
    "\n",
    "    def predict(self, type):\n",
    "        if (type == 'sample_data'):\n",
    "            self.predictor.sample_predict(Evaluator.clf)\n",
    "        elif (type == 'new_data'):\n",
    "            self.predictor.new_predict(Evaluator.clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 38.650s\n",
      "done in 4.427s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.78      0.87      8128\n",
      "          1       0.29      0.83      0.43       872\n",
      "\n",
      "avg / total       0.91      0.78      0.82      9000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.53      0.69       906\n",
      "          1       0.17      0.97      0.30        94\n",
      "\n",
      "avg / total       0.92      0.57      0.65      1000\n",
      "\n",
      "[[477 429]\n",
      " [  3  91]]\n"
     ]
    }
   ],
   "source": [
    "if '__main__' == __name__:\n",
    "    content = io.mmread('D:\\Document\\CS\\Program\\py program\\spam messages\\word_vector.mtx')\n",
    "    with open('D:\\Document\\CS\\Program\\py program\\spam messages\\spam_label.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "    training_data, test_data, training_target, test_target = split_data(content, label)\n",
    "    training_data, test_data = dimensionality_reduction(training_data.todense(), test_data.todense(), type='pca')\n",
    "    evaluator = Evaluator(training_data.todense(), training_target, test_data.todense(), test_target)\n",
    "    evaluator.train()\n",
    "    #evaluator.cross_validation()\n",
    "    evaluator.predict(type='sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
